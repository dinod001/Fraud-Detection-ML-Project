# Why 1 Duplicate Remains After Processing

## The Mystery: How Can 1 Duplicate Still Exist?

You removed duplicates in:
- ‚úÖ **Notebook 01**: 0 duplicates (with all columns including user_id, device_id)
- ‚úÖ **Notebook 02**: Removed 6,840 duplicates (after dropping identifying columns)
- ‚ùì **Final Dataset**: Still 1 duplicate remains!

---

## How This Can Happen: Simple Example

### Scenario: Two Rows That Become Identical After Processing

Imagine you have these two original rows (with all columns):

#### **Row A:**
```
user_id: 100
device_id: DEVICE_X
ip_address: 1.2.3.4
signup_time: 2015-01-01 10:00:00
purchase_time: 2015-01-01 10:05:00
purchase_value: 50
source: SEO
browser: Chrome
sex: M
age: 39
class: 0
```

#### **Row B:**
```
user_id: 200  ‚Üê Different user!
device_id: DEVICE_Y  ‚Üê Different device!
ip_address: 5.6.7.8  ‚Üê Different IP!
signup_time: 2015-02-01 15:00:00  ‚Üê Different time!
purchase_time: 2015-02-01 15:05:00  ‚Üê Different time!
purchase_value: 50  ‚Üê Same!
source: SEO  ‚Üê Same!
browser: Chrome  ‚Üê Same!
sex: M  ‚Üê Same!
age: 39  ‚Üê Same!
class: 0  ‚Üê Same!
```

---

## Step-by-Step Processing

### **Step 1: Notebook 01 - Remove True Duplicates**
- ‚úÖ These are NOT duplicates (different user_id, device_id, timestamps)
- ‚úÖ Both rows pass through
- **Result:** 0 duplicates removed

### **Step 2: Notebook 02 - Feature Engineering**

#### **2.1 Create Features:**
- `account_age_minutes`: 
  - Row A: (2015-01-01 10:05:00 - 2015-01-01 10:00:00) = **5 minutes**
  - Row B: (2015-02-01 15:05:00 - 2015-02-01 15:00:00) = **5 minutes** ‚úÖ Same!
  
- `device_count`: 
  - Row A: Count of DEVICE_X = **1**
  - Row B: Count of DEVICE_Y = **1** ‚úÖ Same!
  
- `user_count_per_device`: 
  - Row A: Unique users on DEVICE_X = **1**
  - Row B: Unique users on DEVICE_Y = **1** ‚úÖ Same!
  
- `country`: 
  - Row A: IP 1.2.3.4 ‚Üí **"United States"**
  - Row B: IP 5.6.7.8 ‚Üí **"United States"** ‚úÖ Same!

#### **2.2 Drop Identifying Columns:**
After dropping `user_id`, `device_id`, `ip_address`, `signup_time`, `purchase_time`:

**Row A becomes:**
```
purchase_value: 50
source: SEO
browser: Chrome
sex: M
age: 39
class: 0
account_age_minutes: 5.0
device_count: 1
user_count_per_device: 1
country: United States
```

**Row B becomes:**
```
purchase_value: 50
source: SEO
browser: Chrome
sex: M
age: 39
class: 0
account_age_minutes: 5.0
device_count: 1
user_count_per_device: 1
country: United States
```

**üéØ They are NOW IDENTICAL!**

#### **2.3 Remove Duplicates:**
- ‚úÖ `drop_duplicates()` should remove one of them
- **But wait...** there might be an edge case!

---

## Why 1 Duplicate Might Still Exist

### **Possible Reasons:**

#### **1. Floating Point Precision Issue**
```python
# Row A: account_age_minutes = 5.0000000001
# Row B: account_age_minutes = 5.0000000002
# These are technically different, but very close!
```

**Solution:** Round the values before duplicate removal:
```python
df_final = df_final.round({'account_age_minutes': 2}).drop_duplicates()
```

#### **2. Missing Value Handling Order**
If `country` has `None` values that get filled with `'Unknown'` AFTER duplicate removal:
```python
# Wrong order:
df_final = df_final.drop_duplicates()  # Removes duplicates
df_final['country'] = df_final['country'].fillna('Unknown')  # Fills None
# Now two rows with None might become identical!
```

**Your code does it correctly:**
```python
df_final['country'] = df['country'].fillna('Unknown')  # Fill first
df_final = df_final.drop_duplicates()  # Then remove duplicates
```

#### **3. Data Type Inconsistency**
```python
# Row A: age = 39 (int)
# Row B: age = 39.0 (float)
# These might not be considered duplicates!
```

#### **4. String vs Numeric Comparison**
```python
# Row A: purchase_value = 50 (int)
# Row B: purchase_value = "50" (string)
# These are different!
```

#### **5. Whitespace or Hidden Characters**
```python
# Row A: source = "SEO"
# Row B: source = "SEO "  (with trailing space)
# These are different!
```

---

## Most Likely Cause: Floating Point Precision

The `account_age_minutes` calculation can produce slightly different floating point values:

```python
# Example:
Row A: (2015-01-01 10:05:00 - 2015-01-01 10:00:00) = 5.0000000001 minutes
Row B: (2015-02-01 15:05:00 - 2015-02-01 15:00:00) = 5.0000000002 minutes

# These are technically different, so drop_duplicates() keeps both!
```

---

## Solution: Fix in Notebook 02

Add rounding before duplicate removal:

```python
# After creating account_age_minutes and before dropping duplicates
df_final['account_age_minutes'] = df_final['account_age_minutes'].round(2)

# Then remove duplicates
df_final = df_final.drop_duplicates()
```

Or ensure all numeric columns are properly typed:

```python
# Ensure consistent data types
df_final['purchase_value'] = df_final['purchase_value'].astype(int)
df_final['device_count'] = df_final['device_count'].astype(int)
df_final['user_count_per_device'] = df_final['user_count_per_device'].astype(int)
df_final['account_age_minutes'] = df_final['account_age_minutes'].round(2)

# Then remove duplicates
df_final = df_final.drop_duplicates()
```

---

## Summary

**What Happened:**
1. Two different users with different devices/IPs/times
2. After feature engineering, they had identical feature values
3. BUT due to floating point precision, `account_age_minutes` was slightly different
4. `drop_duplicates()` saw them as different rows
5. Result: 1 duplicate remains

**Fix:**
- Round `account_age_minutes` before duplicate removal
- Ensure consistent data types
- This will catch the remaining duplicate!

---

## Quick Check

You can verify this by checking:
```python
# In your correlation notebook, check:
duplicates = df[df.duplicated(keep=False)]
print(duplicates[['purchase_value', 'account_age_minutes', 'source', 'browser', 'sex', 'age']])
```

This will show you the duplicate rows and you'll likely see `account_age_minutes` differs by a tiny amount!

